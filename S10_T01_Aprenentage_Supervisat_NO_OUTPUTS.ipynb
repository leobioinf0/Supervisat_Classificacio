{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S10_T01_Aprenentage_Supervisat-NO_OUTPUTS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jg9sGeCrhmBa",
        "zKZEGKl2HMP0",
        "5sGeAR_nfEvt",
        "WLtCtbVZhmBb"
      ],
      "mount_file_id": "1TsAaEeFBBGUh9C4J7Cr2LYmaGihrDw5u",
      "authorship_tag": "ABX9TyNvZN9pPAELGP9BKZdbqQ2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leobioinf0/Supervisat_Classificacio/blob/main/S10_T01_Aprenentage_Supervisat_NO_OUTPUTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyW3pAFnAV-R"
      },
      "source": [
        "# [**Supervised Classification**](https://github.com/leobioinf0/Supervisat_Classificacio)\n",
        "Course: *Data Science amb Python*\n",
        "\n",
        "Sprint: 10. Supervised Learning - classification \n",
        "\n",
        "\n",
        "Task: S10 T01: Supervised Learning - Classification \n",
        "\n",
        "Date: *2022-01-17*\n",
        "\n",
        "[Leo Madsen](https://github.com/leobioinf0)\n",
        "\n",
        "\n",
        "\n",
        "#### Exercises: \n",
        "- Level 1\n",
        "    - Exercise 1: \n",
        "    \n",
        "    Create at least three different classification models to try to best predict DelayedFlights.csv flight delay (ArrDelay). Consider whether the flight is late or not (ArrDelay> 0).\n",
        "    - Exercise 2: \n",
        "    \n",
        "    Compare classification models using accuracy, a confidence matrix, and other more advanced metrics.\n",
        "    - Exercise 3: \n",
        "    \n",
        "    Train them using the different parameters they support.\n",
        "    - Exercise 4: \n",
        "    \n",
        "    Compare your performance using the traint / test or cross-validation approach.\n",
        "- Level 2\n",
        "    - Exercise 5: \n",
        "    \n",
        "    Perform some variable engineering process to improve prediction\n",
        "- Level 3\n",
        "    - Exercise 6: \n",
        "    \n",
        "    Do not use the DepDelay variable when making predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0EtREsdZmH"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg9sGeCrhmBa"
      },
      "source": [
        "## Upgrade modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade lightgbm \n",
        "!pip install --upgrade xgboost\n",
        "!pip install --upgrade feature-engine\n",
        "!pip install --upgrade matplotlib\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade keras\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade pandas"
      ],
      "metadata": {
        "id": "kNs5Iwj3dY3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IBEOJOhmBa"
      },
      "source": [
        "## Load modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## System\n",
        "import os\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "## Data treatment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import math \n",
        "from scipy.stats import sem\n",
        "\n",
        "## Graphics\n",
        "import missingno as msno #missing data visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image, display\n",
        "from tqdm import tqdm_notebook\n",
        "from google.colab import data_table\n",
        "\n",
        "## Metrics\n",
        "from sklearn.metrics import euclidean_distances\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import zero_one_loss\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import max_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "## Data preprocessing\n",
        "from feature_engine.creation import CyclicalTransformer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "## Linear Models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "## Preprocessing and modeling\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "## Neural Network\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "## Tree Models\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "\n",
        "## Ensemble methods\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "## Naive Bayes algorithms\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "## Gradient boosting\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "## Gaussian Process\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "\n",
        "## Support Vector Machine algorithms\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC \n",
        "from sklearn.svm import NuSVC \n",
        "\n",
        "## k-nearest neighbors algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "\n",
        "## semi-supervised learning algorithms.\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "\n",
        "## DummyClassifier makes predictions that ignore the input features.\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "## Calibration of predicted probabilities.\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "## Linear Discriminant Analysis and Quadratic Discriminant Analysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "## Warnings configuration\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "xw4plg_RS_gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable.max_columns = 50\n",
        "data_table.DataTable.num_rows_per_page = 55\n",
        "data_table.DataTable.max_rows = 56"
      ],
      "metadata": {
        "id": "o-k9CgZvdrdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "sns.set(rc={'figure.figsize':(15,7)})"
      ],
      "metadata": {
        "id": "6RFELkDsVqDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKZEGKl2HMP0"
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygodyh3ZIUIt"
      },
      "outputs": [],
      "source": [
        "def timenow():\n",
        "    '''\n",
        "    Return the current time in hours, minutes and seconds.\n",
        "    '''\n",
        "    return datetime.datetime.now().time().replace(microsecond=0)\n",
        "\n",
        "def mk_dir(name):\n",
        "    '''\n",
        "    Create directory with the name passed in the case that it does not exist and return its full path.\n",
        "    '''\n",
        "    path = os.path.join(os.getcwd(), name)\n",
        "    if not os.path.exists(path):\n",
        "        try:\n",
        "            os.makedirs(path, 0o700)\n",
        "        except OSError as e:\n",
        "            if e.errno != errno.EEXIST:\n",
        "                raise\n",
        "        print(\"Directory created: {}\".format(path))\n",
        "    else:\n",
        "        print(\"Directory already existing: {}\".format(path))\n",
        "    return path\n",
        "\n",
        "def rounder(n):\n",
        "    \"\"\"\n",
        "    Round up to the first digit that is not zero\n",
        "    \"\"\"\n",
        "    if n == 0:\n",
        "        return n\n",
        "    else:\n",
        "        k = 1 - int(math.log10(n))\n",
        "        return round(n, 1 if n > 1 else k)\n",
        "\n",
        "def describer(dataframe):\n",
        "    \"\"\"\n",
        "    Description of dataframe\n",
        "    \"\"\"\n",
        "    desc_df = round(dataframe.describe(include=\"all\"),2).T\n",
        "    desc_df[\"unique\"]=dataframe.nunique()\n",
        "    desc_df[\"NullAny\"]= dataframe.isnull().any()\n",
        "    desc_df[\"NullSum\"]=dataframe.isnull().sum()\n",
        "    desc_df[\"NullPct\"]=(dataframe.isnull().sum()/len(dataframe)*100).apply(rounder)\n",
        "    desc_df[\"dtypes\"]=dataframe.dtypes\n",
        "    desc_df.sort_values(by=\"dtypes\", inplace=True)\n",
        "    return(desc_df)\n",
        "\n",
        "\n",
        "def kfcv_evaluator(X, y, model, n_s):\n",
        "    '''\n",
        "    Evaluate a R2 score by K-Folds cross-validation of a given model.\n",
        "    '''\n",
        "    # prepare the cross-validation procedure\n",
        "    cv = KFold(n_splits=n_s, random_state=1, shuffle=True)\n",
        "    # evaluate model\n",
        "    scores = cross_val_score(model, X, y, scoring='r2', cv=cv, verbose=3)\n",
        "    return scores.round(4)\n",
        "\n",
        "def kfcv_evaluator_class(X, y, model, n_s):\n",
        "    '''\n",
        "    Evaluate a Accuracy score by K-Folds cross-validation of a given model.\n",
        "    '''\n",
        "    # prepare the cross-validation procedure\n",
        "    cv = KFold(n_splits=n_s, random_state=1, shuffle=True)\n",
        "    # evaluate model\n",
        "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n",
        "    return scores.round(4)\n",
        "\n",
        "def rkfcv_evaluator(X, y, model, n_s, n_r):\n",
        "    '''\n",
        "    Evaluate a R2 score by  Repeated K-Folds cross-validation of a given model.\n",
        "    '''\n",
        "    cv = RepeatedKFold(n_splits=n_s, n_repeats=n_r, random_state=1)\n",
        "    scores = cross_val_score(model, X, y, scoring='r2', cv=cv, verbose=3)\n",
        "    return scores.round(4)\n",
        "\n",
        "def rkfcv_evaluator_class(X, y, model, n_s, n_r):\n",
        "    '''\n",
        "    Evaluate a Accuracy score by  Repeated K-Folds cross-validation of a given model.\n",
        "    '''\n",
        "    cv = RepeatedKFold(n_splits=n_s, n_repeats=n_r, random_state=1)\n",
        "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv)\n",
        "    return scores.round(4)\n",
        "\n",
        "def numToTime(x):\n",
        "    \"\"\"\n",
        "    Convert aeronautical time to datetime.time object\n",
        "    \"\"\"\n",
        "    from datetime import time\n",
        "    if pd.isnull(x):\n",
        "        return np.nan\n",
        "    else:\n",
        "        x = str(int(x))\n",
        "        h = x[:-2]\n",
        "        if h == \"\" or h == \"24\":\n",
        "            h = \"0\"\n",
        "        m = x[-2:]\n",
        "        return time(int(h), int(m))\n",
        "\n",
        "def to_flat(regular_list):\n",
        "    '''\n",
        "    Flatten a given list.\n",
        "    '''\n",
        "    flat_list = [item for sublist in regular_list for item in sublist]\n",
        "    return(flat_list)\n",
        "\n",
        "def class_metrics(model, name, X_train, X_test, y_train, y_test, plt_path):\n",
        "    '''\n",
        "    Evaluate a classification model and calculate various metrics. Return the metrics in a dictionary\n",
        "    '''\n",
        "    y_pred = model.predict(X_test)\n",
        "    conf_matrix = confusion_matrix(y_test,y_pred)\n",
        "    \n",
        "    scores = {'Model': name,\n",
        "        'Accuracy':accuracy_score(y_test,y_pred).round(4),\n",
        "        'B_Accuracy':balanced_accuracy_score(y_test,y_pred).round(4),\n",
        "        'F1_Score':f1_score(y_test,y_pred).round(4),\n",
        "        'Precision':precision_score(y_test,y_pred).round(4),\n",
        "        'Recall':recall_score(y_test,y_pred).round(4),\n",
        "        'Specificity' : recall_score(y_test, y_pred, pos_label=0).round(4),\n",
        "        'FP_rate' : (1 - recall_score(y_test, y_pred, pos_label=0)).round(4),\n",
        "        'Misclass_rate' : zero_one_loss(y_test, y_pred).round(4),\n",
        "        'Misclass' : zero_one_loss(y_test, y_pred, normalize=False).round(4),\n",
        "        'TP' : conf_matrix[1, 1].round(4),\n",
        "        'TN' : conf_matrix[0, 0].round(4),\n",
        "        'FP' : conf_matrix[0, 1].round(4),\n",
        "        'FN' : conf_matrix[1, 0].round(4)}\n",
        "    \n",
        "    print(\"Confusion matrix :\\n{}\".format(conf_matrix))\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test,y_pred,normalize='true')\n",
        "    print(\"Normalized Confusion matrix:\\n{}\".format(conf_matrix))\n",
        "    \n",
        "\n",
        "    scores_table=pd.DataFrame(data=scores.items(), columns=[\"METRIC\", \"VALUES\"])\n",
        "    desc_table=pd.DataFrame(data=model.get_params().items(), columns=[\"PARAMS\", \"VALUES\"])\n",
        "\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='g', ax=ax, square=True, ); \n",
        "    desc_t = ax.table(cellText=desc_table.values,\n",
        "                colLabels=desc_table.columns,\n",
        "                bbox=[1.5, 0, 1.0, 1.0])\n",
        "    desc_t.auto_set_column_width(col=list(range(len(desc_table.columns))))\n",
        "\n",
        "    scores_t = ax.table(cellText=scores_table.values,\n",
        "                colLabels=scores_table.columns,\n",
        "                bbox=[-1.1, 0, 1.0, 1.0])\n",
        "    scores_t.auto_set_column_width(col=list(range(len(scores_table.columns))))\n",
        "    plt.title(name)\n",
        "    plt.savefig(plt_path)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    return scores\n",
        "\n",
        "def reg_metrics(name, model, X_test, y_true):\n",
        "    '''\n",
        "    Evaluate a regression model and calculate various metrics. Return the metrics in a dataframe\n",
        "    '''\n",
        "    y_pred = model.predict(X_test)\n",
        "    me = max_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    ev = explained_variance_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    medae = median_absolute_error(y_true, y_pred)\n",
        "    reg_metrics_df = pd.DataFrame(data=[[r2,mse, mae,  mape, ev,me, medae]], \n",
        "                                  columns=[\"R2\",\"MSE\", \"MAE\", \"MAPE\", \"EV\", \"ME\", \"MedAE\"], \n",
        "                                  index=[name])\n",
        "    return(reg_metrics_df)\n",
        "\n",
        "def rkfcv_to_df(results):\n",
        "    \"\"\"\n",
        "    Tabulate the scores of a Given  dictionary with Repeated K-Folds cross-validation scores.\n",
        "    And save the table to a .csv file. \n",
        "    \"\"\"\n",
        "    df = pd.DataFrame.from_dict({(i,j): rkfcv_results[i][j] \n",
        "                        for i in rkfcv_results.keys() \n",
        "                        for j in rkfcv_results[i].keys()},\n",
        "                    orient='index')\n",
        "    df.reset_index(level=[0,1], inplace=True)\n",
        "    df.columns = [\"model\",\"n_splits\"] + [str(i) for i in n_repeats]\n",
        "    df.to_csv(rkfcv_filename_path+\".csv\")\n",
        "    return df\n",
        "    \n",
        "def plot_rkfcv(df):\n",
        "    \"\"\"\n",
        "    Boxplot of Repeated K-Folds cross-validation scores.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(nrows=df.model.nunique(), \n",
        "                            ncols=df.n_splits.nunique(),\n",
        "                            figsize=(df.n_splits.nunique()*8,df.model.nunique()*4),\n",
        "                            sharex=True, sharey = True)\n",
        "\n",
        "    for i, modl in enumerate(df.model.unique()):\n",
        "        for j, splts in enumerate(df.n_splits.unique()):\n",
        "            data = df[(df.model==modl) & (df.n_splits==splts)][[str(i) for i in n_repeats]].apply(to_flat)\n",
        "            axes[i][j].boxplot(data, showmeans=True)\n",
        "            ttl=\"{} ({} splits)\".format(modl, splts)\n",
        "            axes[i][j].set_title(ttl, fontweight = \"bold\")\n",
        "\n",
        "    fig.suptitle(t='Number of Repeats Performance Comparison in \\nRepeated k-Fold Cross-Validation ', \n",
        "                fontsize = 18, fontweight = \"bold\")\n",
        "    fig.supxlabel('Nº repeats',fontsize = 18, fontweight = \"bold\")\n",
        "    fig.supylabel('Score',fontsize = 18, fontweight = \"bold\")\n",
        "\n",
        "    filename = \"All_Repeated_K-fold_cv.png\"\n",
        "    plt.savefig(os.path.join(exe04_plots_path, filename))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sGeAR_nfEvt"
      },
      "source": [
        "## Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltKVdkMN-O2U"
      },
      "outputs": [],
      "source": [
        "# working directory\n",
        "\n",
        "cwd = './'\n",
        "#cwd = '/content/drive/MyDrive/Data_Science_amb_Python/Sprint10-Aprenentatge_Supervisat_Classificacio/Supervisat_Classificacio/'\n",
        "os.chdir(cwd)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLtCtbVZhmBb"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOZaDWKOhPTh"
      },
      "outputs": [],
      "source": [
        "# Read a smaller version of the dataset from raw.githubusercontent\n",
        "filepath = \"https://raw.githubusercontent.com/leobioinf0/Supervitat_Regressio/main/DelayedFlights3mb.csv\"\n",
        "df = pd.read_csv(filepath, index_col=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYUrk4CJaovU"
      },
      "source": [
        "____________________\n",
        "This dataset is composed by the following variables: \n",
        "\n",
        "1.\t**Year**\t2016\n",
        "2.\t**Month**\t1-12\n",
        "3.\t**DayofMonth**\t1-31\n",
        "4.\t**DayOfWeek**\t1 (Monday) - 7 (Sunday)\n",
        "5.\t**DepTime**\tactual departure time (local, hhmm)\n",
        "6.\t**CRSDepTime**\tscheduled departure time (local, hhmm)\n",
        "7.\t**ArrTime**\tactual arrival time (local, hhmm)\n",
        "8.\t**CRSArrTime**\tscheduled arrival time (local, hhmm)\n",
        "9.\t**UniqueCarrier**\tunique carrier code\n",
        "10.\t**FlightNum**\tflight number\n",
        "11.\t**TailNum** plane tail number: aircraft registration, unique aircraft identifier\n",
        "12.\t**ActualElapsedTime**\tin minutes\n",
        "13.\t**CRSElapsedTime**\tin minutes\n",
        "14.\t**AirTime**\tin minutes\n",
        "15.\t**ArrDelay**\tarrival delay, in minutes: **A flight is counted as \"on time\" if it operated less than 15 minutes later the scheduled time shown in the carriers' Computerized Reservations Systems (CRS).** \n",
        "16.\t**DepDelay**\tdeparture delay, in minutes\n",
        "17.\t**Origin**\torigin IATA airport code\n",
        "18.\t**Dest**\tdestination IATA airport code\n",
        "19.\t**Distance**\tin miles\n",
        "20.\t**TaxiIn**\ttaxi in time, in minutes\n",
        "21.\t**TaxiOut**\ttaxi out time in minutes\n",
        "22.\t**Cancelled**\t*was the flight cancelled\n",
        "23.\t**CancellationCode**\treason for cancellation (A = carrier, B = weather, C = NAS, D = security)\n",
        "24.\t**Diverted**\t1 = yes, 0 = no\n",
        "25.\t**CarrierDelay**\tin minutes: Carrier delay is within the control of the air carrier. Examples of occurrences that may determine carrier delay are: aircraft cleaning, aircraft damage, awaiting the arrival of connecting passengers or crew, baggage, bird strike, cargo loading, catering, computer, outage-carrier equipment, crew legality (pilot or attendant rest), damage by hazardous goods, engineering inspection, fueling, handling disabled passengers, late crew, lavatory servicing, maintenance, oversales, potable water servicing, removal of unruly passenger, slow boarding or seating, stowing carry-on baggage, weight and balance delays.\n",
        "26.\t**WeatherDelay**\tin minutes: Weather delay is caused by extreme or hazardous weather conditions that are forecasted or manifest themselves on point of departure, enroute, or on point of arrival.\n",
        "27.\t**NASDelay**\tin minutes: Delay that is within the control of the National Airspace System (NAS) may include: non-extreme weather conditions, airport operations, heavy traffic volume, air traffic control, etc. \n",
        "28.\t**SecurityDelay**\tin minutes: Security delay is caused by evacuation of a terminal or concourse, re-boarding of aircraft because of security breach, inoperative screening equipment and/or long lines in excess of 29 minutes at screening areas.\n",
        "29.\t**LateAircraftDelay**\tin minutes: Arrival delay at an airport due to the late arrival of the same aircraft at a previous airport. The ripple effect of an earlier delay at downstream airports is referred to as delay propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPG4qe5plWcy"
      },
      "outputs": [],
      "source": [
        "# Full description of dataframe\n",
        "describer(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXoQ8pZIhmBc"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6GHt5Q9Fre5"
      },
      "source": [
        "## Missing in ArrDelay\n",
        "\n",
        "- We eliminate the flights that do not have data in the response variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2RgwNFlsANW"
      },
      "outputs": [],
      "source": [
        "df = df[df['ArrDelay'].notna()].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Judm-xDMlXgO"
      },
      "source": [
        "## Cancelled/CancellationCode\n",
        "\n",
        "- First we eliminate all the flights that were canceled since they do not provide information.\n",
        "- Then we remove the columns [\"Canceled\", \"CancellationCode\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSKG5x1YklFr"
      },
      "outputs": [],
      "source": [
        "df = df[df.Cancelled==0].copy()\n",
        "df.drop(labels=[\"Cancelled\", \"CancellationCode\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLaDySd8m-ST"
      },
      "source": [
        "## Diverted\n",
        "- First we eliminate all the flights that were diverted since they do not provide information.\n",
        "- Then we remove the column [\"Diverted\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbLlnfXRlW7Z"
      },
      "outputs": [],
      "source": [
        "df = df[df.Diverted==0].copy()\n",
        "df.drop(labels=[\"Diverted\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reindex"
      ],
      "metadata": {
        "id": "Mm_vKQcxI_Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "ZZcWNdxYJBuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhzdEE3n-_uw"
      },
      "source": [
        "## Missing values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().any()"
      ],
      "metadata": {
        "id": "3dV49lSqHCEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcbC48OF-_uw"
      },
      "source": [
        "msno.matrix(df);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variables that still contain nan are the ones that describe the delay.\n",
        "We assume that these nan are due to the \"ArrDelay\" being low enough not to be described, so we will replace them with zeros.\n"
      ],
      "metadata": {
        "id": "UOHLOf4jH83v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "QGBpAv-GKm5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om90xHFb-_ux"
      },
      "source": [
        "df.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jc_5kUE-_um"
      },
      "source": [
        "## Target Binarizing\n",
        " \n",
        "Create a variable Target depending on whether the flight was late or not (ArrDelay> 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Means Delay, 0 Means no Delay\n",
        "transformer = Binarizer()\n",
        "binary = transformer.fit_transform(df[['ArrDelay']])\n",
        "df['ArrDelay'] = pd.Series(binary.flatten()).astype(int) #df['ArrDelay'] = df['ArrDelay'].apply(lambda x: 0 if x <=0 else 1)"
      ],
      "metadata": {
        "id": "Tf2dUOc7PGD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the dataset is unbalanced. Although we know that this is a problem, we will solve it in exercise 5, so we can see the effect that the balance of the data has on the predictions."
      ],
      "metadata": {
        "id": "Ddq3-cTFjfpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['ArrDelay'].value_counts(normalize=True).round(2))\n",
        "print(df['ArrDelay'].value_counts())"
      ],
      "metadata": {
        "id": "pY4b7IJ-V1cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYiwGk0WNxLR"
      },
      "source": [
        "## Save Preproceced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0qvvoP4DFwD"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"DelayedFlights_Processed.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqkBxTqIhmBZ"
      },
      "source": [
        "#  Exercise 1: \n",
        "  - Create at least three different classification models to try to best predict DelayedFlights.csv flight delay (ArrDelay). Consider whether the flight is late or not (ArrDelay> 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu8id4lj9XY4"
      },
      "source": [
        "Classification models we are going to use:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [DummyClassifier(),\n",
        "               KNeighborsClassifier(),\n",
        "               SVC(),\n",
        "               DecisionTreeClassifier(),\n",
        "               RandomForestClassifier(),\n",
        "               MLPClassifier(),\n",
        "               AdaBoostClassifier(),\n",
        "               GaussianNB(),\n",
        "               QuadraticDiscriminantAnalysis(),\n",
        "               NearestCentroid(),\n",
        "               BaggingClassifier(),\n",
        "               Perceptron(),\n",
        "               PassiveAggressiveClassifier(),\n",
        "               ExtraTreeClassifier(),\n",
        "               LabelSpreading(),\n",
        "               LabelPropagation(),\n",
        "               LogisticRegression(),\n",
        "               SGDClassifier(),\n",
        "               LinearDiscriminantAnalysis(),\n",
        "               CalibratedClassifierCV(),\n",
        "               RidgeClassifier(),\n",
        "               RidgeClassifierCV(),\n",
        "               LinearSVC(),\n",
        "               BernoulliNB(),\n",
        "               ExtraTreesClassifier(),\n",
        "               LGBMClassifier(),\n",
        "               XGBClassifier()]"
      ],
      "metadata": {
        "id": "cnlwfzmeVZfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZErWlXH2R6CN"
      },
      "source": [
        "We read the already processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_7wJWG-sIu-"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"DelayedFlights_Processed.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ02BtDfTRTO"
      },
      "source": [
        "We create the directory in which we will save the results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exe01_path = mk_dir(\"exe01\")\n",
        "exe01_models_path = mk_dir(\"exe01/exe01_models\")\n",
        "exe01_tables_path = mk_dir(\"exe01/exe01_tables\")\n",
        "exe01_plots_path = mk_dir(\"exe01/exe01_plots\")"
      ],
      "metadata": {
        "id": "9I1JOjVmUFZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDzJGmsz-_uv"
      },
      "source": [
        "Transform Categorical Variables into ordinal values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZEBFb7e-_uv"
      },
      "source": [
        "encoder = OrdinalEncoder()\n",
        "df[['UniqueCarrier', 'Origin', 'Dest', 'TailNum']]= encoder.fit_transform(df[['UniqueCarrier', 'Origin', 'Dest', 'TailNum']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide data into Explanatory and Response variables. "
      ],
      "metadata": {
        "id": "rHZBXLrqE9ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['ArrDelay'], axis = 'columns')\n",
        "y = df['ArrDelay']"
      ],
      "metadata": {
        "id": "wCZZQKq9B_Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We scale the data and divide it into training and test sets\n",
        "\n",
        "We use MinMaxScaler because we do not assume that the shape of all our features follows a normal distribution."
      ],
      "metadata": {
        "id": "Ew_p1M1qRIF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = MinMaxScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "2AxLGvUYCKUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=130)"
      ],
      "metadata": {
        "id": "0ucdHEt6CQU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59LaxQnNvaNS"
      },
      "source": [
        "\n",
        "\n",
        "1.   For each model we create the name of the model and the name of the file where the trained model will be saved.\n",
        "2.   If the file already exists we load it and if it does not exist we train it and save it.\n",
        "\n",
        "3.  Then we evaluate the model and store the metrics in a list.\n",
        "\n",
        "4.  Each model with its metrics are tabulated in a dataframe and sorted in descending order according to its Accuracy.\n",
        "\n",
        "5.  Finally we save the table in a file."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train models and make predictions."
      ],
      "metadata": {
        "id": "vLPYu9U1N6c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_lst = []\n",
        "for model in classifiers:\n",
        "    # Model name \n",
        "    model_str = str(model).split(\"(\")[0]\n",
        "    model_name = '{}-{}'.format(\"01\", model_str)\n",
        "    # File name\n",
        "    model_filename = model_name + \".pkl\"\n",
        "    model_filename_path = os.path.join(exe01_models_path, model_filename)\n",
        "    if os.path.isfile(model_filename_path):\n",
        "        # load existing model\n",
        "        print(\"\\n{}: Loading model:\\t{}\".format(timenow(), model_filename))\n",
        "        model = pickle.load(open(model_filename_path, 'rb'))\n",
        "    else:\n",
        "        # Fit model\n",
        "        print(\"\\n{}: Start Fitting model:\\t{}\".format(timenow(), model_name))\n",
        "        tiempo_inicio_fit = datetime.datetime.now()\n",
        "        model.fit(X_train, y_train)\n",
        "        fit_time = datetime.datetime.now() - tiempo_inicio_fit\n",
        "        print(\"Fitting time:\\t{}\".format(fit_time))\n",
        "\n",
        "        # save the model to disk\n",
        "        pickle.dump(model, open(model_filename_path, 'wb'))\n",
        "\n",
        "    # Evaluate model\n",
        "    plt_path = os.path.join(exe01_plots_path, model_name + \"_conf.png\")\n",
        "    print(\"\\n{}: Evaluate model:\\n\".format(timenow()))\n",
        "    metrics_lst.append(class_metrics(model, model_name, X_train, X_test, y_train, y_test, plt_path))\n",
        "# Tabualate resulst\n",
        "class_metrics_df = pd.DataFrame(metrics_lst)\n",
        "class_metrics_df.set_index(keys=\"Model\", inplace=True)\n",
        "class_metrics_df.sort_values(by=[\"Accuracy\"], ascending=False, inplace=True)\n",
        "# Save results\n",
        "class_metrics_df.to_csv(os.path.join(exe01_tables_path, \"01-classification_metrics.csv\"))"
      ],
      "metadata": {
        "id": "rtblap4oW3lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTLRhtwrFCW_"
      },
      "source": [
        "# Exercise 2: \n",
        "  - Compare classification models using accuracy, a confidence matrix, and other more advanced metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14noYvk2i9zG"
      },
      "source": [
        "We create the directory in which we will save the results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exe02_path = mk_dir(\"exe02\")\n",
        "exe02_plots_path = mk_dir(\"exe02/exe02_plots\")"
      ],
      "metadata": {
        "id": "6ukRg1h5j5ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We read the results of exercise 1 and plot them."
      ],
      "metadata": {
        "id": "iPeJmc5JErVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_01_path = os.path.join(exe01_tables_path, \"01-classification_metrics.csv\")\n",
        "metrics_lst = []\n",
        "for metrics_file in [metrics_01_path]:\n",
        "    all_metrics = pd.read_csv(metrics_file, index_col=0)\n",
        "    metrics_lst.append(all_metrics)\n",
        "final_metrics_df_1 = pd.concat(metrics_lst)\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(final_metrics_df_1, num_rows_per_page=final_metrics_df_1.shape[0])"
      ],
      "metadata": {
        "id": "MULm7SN_yLZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for metric in final_metrics_df_1.columns:\n",
        "    if metric in [\"FP_rate\",\"Misclass_rate\",\"Misclass\",\"FP\",\"FN\"]:\n",
        "        ascend=True\n",
        "    else:\n",
        "        ascend=False\n",
        "    final_metrics_df_1.sort_values(by=metric, inplace=True, ascending=ascend)\n",
        "    plt.figure(figsize = (15,final_metrics_df_1.shape[0]/4))\n",
        "    sns.barplot(data=final_metrics_df_1, x = metric, y = final_metrics_df_1.index)\n",
        "    plt.title(metric)\n",
        "    plt_path = os.path.join(exe02_plots_path, metric + \"_summary.png\")\n",
        "    plt.savefig(plt_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0wpaUWjeyLZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9od55gr_S-u"
      },
      "source": [
        "# Exercise 3: \n",
        "  - Train them using the different parameters they support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K250NjHOFfmO"
      },
      "source": [
        "We create the directory in which we will save the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-w1-rHC2L2D"
      },
      "outputs": [],
      "source": [
        "mk_dir(\"exe03\")\n",
        "exe03_models_path = mk_dir(\"exe03/exe03_models\")\n",
        "exe03_tables_path = mk_dir(\"exe03/exe03_tables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W75VRoTFfRdX"
      },
      "source": [
        "- [***LogisticRegression***](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression)\n",
        "    - **solver** (Algorithm to use in the optimization problem.):\n",
        "        - newton-cg\n",
        "        - lbfgs\n",
        "        - liblinear\n",
        "        - sag\n",
        "        - saga\n",
        "    - **C** (Inverse of regularization strength.)\n",
        "        - gbtree\n",
        "        - gblinear\n",
        "    - **penalty** (Specify the norm of the penalty):\n",
        "        - none\n",
        "        - l2\n",
        "        - l1\n",
        "        - elasticnet\n",
        "\n",
        "- [***RidgeClassifier***](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html?highlight=ridgeclassifier#sklearn.linear_model.RidgeClassifier)\n",
        "    - **alpha** (Regularization strength):\n",
        "        - 0.1\n",
        "        - 0.2\n",
        "        - 0.3\n",
        "        - 0.4\n",
        "        - 0.5\n",
        "        - 0.6\n",
        "        - 0.7\n",
        "        - 0.8\n",
        "        - 0.9\n",
        "        - 1.0\n",
        "\n",
        "- [***KNeighborsClassifier***](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier)\n",
        "    - **weights** (Weight function used in prediction.):\n",
        "        - uniform\n",
        "        - distance\n",
        "    - **metric** (The distance metric to use for the tree.):\n",
        "        - euclidean\n",
        "        - manhattan\n",
        "        - minkowski\n",
        "\n",
        "- [***SVC***](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC)\n",
        "    - **kernel** (Specifies the kernel type to be used in the algorithm.):\n",
        "        - linear\n",
        "        - poly\n",
        "        - rbf\n",
        "        - sigmoid\n",
        "    - **C** (Regularization parameter.):\n",
        "        - 1\n",
        "        - 10\n",
        "        - 100\n",
        "    - **gamma** (Kernel coefficient):\n",
        "        - scale\n",
        "        - auto\n",
        "        \n",
        "    \n",
        "- [***BaggingClassifier***](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html?highlight=baggingclassifier#sklearn.ensemble.BaggingClassifier)\n",
        "    - **n_estimators** (The number of base estimators in the ensemble.):\n",
        "        - 10\n",
        "        - 100\n",
        "        - 1000\n",
        "\n",
        "- [***RandomForestClassifier***](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)\n",
        "    - **max_depth** (The maximum depth of the tree.):\n",
        "        - 3\n",
        "        - 5\n",
        "        - 10\n",
        "        - None\n",
        "    - **min_samples_split** (The minimum number of samples required to split an internal node):\n",
        "        - 2\n",
        "        - 5\n",
        "        - 10\n",
        "    - **max_features** (The number of features to consider when looking for the best split):\n",
        "        - sqrt\n",
        "        - log2\n",
        "        - None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_classifiers = [LogisticRegression(random_state = 130), \n",
        "                      RidgeClassifier(random_state = 130),\n",
        "                      KNeighborsClassifier(), \n",
        "                      SVC(random_state = 130),\n",
        "                      BaggingClassifier(random_state = 130),\n",
        "                      RandomForestClassifier(random_state = 130)]\n",
        "\n",
        "LogisticRegression_param_grid = {\"solver\":['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "                                 \"C\":[10, 1.0, 0.1, 0.01],\n",
        "                                 \"penalty\":['l1', 'l2', 'elasticnet', 'none']}\n",
        "RidgeClassifier_param_grid = {\"alpha\":[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
        "KNeighborsClassifier_param_grid = {\"weights\":['uniform', 'distance'],\n",
        "                                 \"metric\":['euclidean', 'manhattan', 'minkowski']}\n",
        "SVC_param_grid = {\"C\":[1, 10, 100],\n",
        "                  \"kernel\":['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "                  \"gamma\":['scale', 'auto']}        \n",
        "BaggingClassifier_param_grid = {\"n_estimators\":[10, 100, 1000]}  \n",
        "RandomForestClassifier_param_grid = {\"max_depth\":[3, 5, 10, None],\n",
        "                                 \"min_samples_split\":[2, 5, 10],\n",
        "                                 \"max_features\":['sqrt', 'log2', None]}     \n",
        "param_grids = [LogisticRegression_param_grid,\n",
        "               RidgeClassifier_param_grid,\n",
        "               KNeighborsClassifier_param_grid,\n",
        "               SVC_param_grid,\n",
        "               BaggingClassifier_param_grid,\n",
        "               RandomForestClassifier_param_grid]                                                   "
      ],
      "metadata": {
        "id": "yr-I03EhWy4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   First we define the estimator that we will use in the search for parameters.\n",
        "\n",
        "1.   Create a dictionary with parameters names as keys and lists of parameter settings to try as values.\n",
        "\n",
        "1.   Then we define the name of the GridSearchCV object, the name of the file where the search result is saved and the path of the file.\n",
        "\n",
        "1.   If the GridSearchCV has already been trained and saved, that is, if the file already exists, it is read.\n",
        "\n",
        "1.   If it has not been previously trained, we define the GridSearchCV object with the estimator and the indicated parameters.\n",
        "\n",
        "1.   Then it is trained, and saved to disk.\n",
        "\n",
        "1.   Finally we tabulate the results and save the results in a csv file.\n",
        "1.   In the last step we show the table with the results (they are not ordered)"
      ],
      "metadata": {
        "id": "pRwTSC0KM8Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model, param_grid in zip(selected_classifiers, param_grids):\n",
        "    print(model)\n",
        "    print(param_grid)\n",
        "    # GridSearchCV name and filename\n",
        "    model_name = '{}-{}'.format(\"03\", \"GSCV_\" + str(model).split(\"(\")[0])\n",
        "    model_filename = model_name + \".pkl\"\n",
        "    model_filename_path = os.path.join(exe03_models_path, model_filename)\n",
        "\n",
        "    if os.path.isfile(model_filename_path):\n",
        "        # load existing model\n",
        "        print(\"\\n{}: Loading model:\\t{}\".format(timenow(), model_filename))\n",
        "        GSCV_model = pickle.load(open(model_filename_path, 'rb'))\n",
        "    else:\n",
        "        # set GridSearchCV\n",
        "        GSCV_model = GridSearchCV(estimator=model, \n",
        "                                  param_grid=param_grid, \n",
        "                                  verbose=1, \n",
        "                                  cv = 2)\n",
        "        print(\"\\n{}: Start Fitting model:\\t{}\".format(timenow(), model_name))\n",
        "        tiempo_inicio_fit = datetime.datetime.now()\n",
        "        # Fit GridSearchCV\n",
        "        GSCV_model.fit(X, y)\n",
        "        print(\"\\n{}: Finish. Fitting time:\\t{}\".format(timenow(), datetime.datetime.now() - tiempo_inicio_fit))\n",
        "        # save the GridSearchCV to disk\n",
        "        pickle.dump(GSCV_model, open(model_filename_path, 'wb'))\n",
        "\n",
        "    # Results filename\n",
        "    GSCV_results_filename = model_name + \"_metrics.csv\"\n",
        "    # Results dataframe\n",
        "    GSCV_model_results = pd.DataFrame(GSCV_model.cv_results_)\n",
        "    GSCV_model_results.index =  model_name + '-'+GSCV_model_results.filter(regex=\"param_\").astype(str).agg('_'.join, axis=1)\n",
        "    GSCV_model_results.sort_values(by=\"rank_test_score\", inplace=True)\n",
        "    cols = list(GSCV_model_results.columns)\n",
        "    cols.reverse()\n",
        "    GSCV_model_results = GSCV_model_results[cols]\n",
        "    # Results to file\n",
        "    print(\"\\n{}: Save GSCV results in:\\t{}\".format(timenow(), GSCV_results_filename))\n",
        "    GSCV_model_results.to_csv(os.path.join(exe03_tables_path, GSCV_results_filename))\n",
        "    display(GSCV_model_results)"
      ],
      "metadata": {
        "id": "o_SmF-_45BtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a similar way to the exercise 2, we read the results of each GridSearchCV.\n",
        "\n",
        "For each model we plot its Accuracy score hued by each by its Grid Searched Parameter. "
      ],
      "metadata": {
        "id": "4vEJ7xX5MJOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for GSCV_results_filename in os.listdir(exe03_tables_path):\n",
        "    GSCV_results_df = pd.read_csv(os.path.join(exe03_tables_path,GSCV_results_filename), index_col=0)\n",
        "    GSCV_name = GSCV_results_filename.split(\".\")[0]\n",
        "    print('\\n\\t',GSCV_name,'\\n')\n",
        "    if GSCV_name == \"03-GSCV_RandomForestClassifier_metrics\":\n",
        "        GSCV_results_df.fillna(\"none\", inplace=True)\n",
        "    params = GSCV_results_df.filter(regex=\"param_\").columns.to_list()\n",
        "    if len(params) == 1:\n",
        "        sns.barplot(data=GSCV_results_df, x=\"mean_test_score\", y=GSCV_results_df.index)\n",
        "        plt.show()\n",
        "    else:\n",
        "        for parm_to_hue in params:\n",
        "            plt.figure(figsize = (15,GSCV_results_df.shape[0]/3))\n",
        "            sns.barplot(data=GSCV_results_df, x=\"mean_test_score\", y=GSCV_results_df.index, hue=parm_to_hue)\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "w1Z6WtNuRJiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6tZMjRWihw-"
      },
      "source": [
        "- LogisticRegression:\n",
        "     - **solver**: `newton-cg` and `lbfgs` produces the best results.\n",
        "     - **C**: `gbtree` and `gblinear` produce similar results.\n",
        "     - **penalty**: `none` produces the best results, `elasticnet` does not get results.\n",
        "- RidgeClassifier:\n",
        "     - **alpha**: all produce similar results.\n",
        "- KNeighboursClassifier:\n",
        "     - **weights**: all produce similar results.\n",
        "     - **metric**: all produce similar results.\n",
        "- CVS:\n",
        "     - **kernel**: `linear` > `rbf` > `poly` > `sigmoid`.\n",
        "     - **C**: the higher the value, the better results are produced.\n",
        "     - **gamma**: `scale` and `auto` all produce similar results.\n",
        "- BaggingClassifier:\n",
        "     - **n_estimators**: the higher the value, the better results are produced.\n",
        "- RandomForestClassifier:\n",
        "     - **max_depth**: the higher the value, the better results are produced.\n",
        "     - **min_samples_split**: all produce similar results.\n",
        "     - **max_features**: `None` produces the best results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We order the model by the Accuracy score in ascending order and finally we show the table and and plot them."
      ],
      "metadata": {
        "id": "iOUG8DsP3bYS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFVVEKAt5oLp"
      },
      "outputs": [],
      "source": [
        "exe03_tables_path = mk_dir(\"exe03/exe03_tables\")\n",
        "metrics_lst = []\n",
        "for GSCV_results_filename in os.listdir(exe03_tables_path):\n",
        "    GSCV_results_df = pd.read_csv(os.path.join(exe03_tables_path,GSCV_results_filename), index_col=0)\n",
        "    metrics_lst.append(GSCV_results_df[[\"mean_test_score\"]])\n",
        "all_metrics_df = pd.concat(metrics_lst)\n",
        "all_metrics_df.sort_values(by=[\"mean_test_score\"], ascending=[False], inplace=True)\n",
        "display(all_metrics_df)\n",
        "plt.figure(figsize = (20,int(all_metrics_df.shape[0]/3.5)))\n",
        "sns.barplot(data=all_metrics_df, x = \"mean_test_score\", y = all_metrics_df.index)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best results were obtained by the LogisticRegression estimator with the following parameters:\n",
        "- solver: newton-cg\n",
        "- penalty: none"
      ],
      "metadata": {
        "id": "PdSMNiObqof7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQALr-cEFqru"
      },
      "source": [
        "# Exercise 4: \n",
        "  - Compare your performance using the train / test approach or using all data (internal validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQyr1OZAROwa"
      },
      "source": [
        "We create the directory in which we will save the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZUchvq2MQCT"
      },
      "outputs": [],
      "source": [
        "mk_dir(\"exe04\")\n",
        "exe04_plots_path = mk_dir(\"exe04/exe04_plots\")\n",
        "exe04_tables_path = mk_dir(\"exe04/exe04_tables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aY1OT42n57t"
      },
      "source": [
        "## Train Test approach\n",
        "\n",
        "We read the results from the exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_metrics_df = pd.read_csv(os.path.join(exe01_tables_path, \"01-classification_metrics.csv\"), index_col=0)\n",
        "display(class_metrics_df.round(5))"
      ],
      "metadata": {
        "id": "4y62olA31umB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,int(class_metrics_df.shape[0]/3.5)))\n",
        "sns.barplot(data=class_metrics_df, x = \"Accuracy\", y = class_metrics_df.index)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zSobr-5buZUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohza3rI_zzD-"
      },
      "source": [
        "## k-Fold Cross-Validation (internal validation)\n",
        "\n",
        "Split dataset into k consecutive folds.\n",
        "\n",
        "Each fold is then used once as a validation while the k-1 remaining folds form the training set.\n",
        "\n",
        "We will be increasing the K from 2 to 0 to see the effect on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PuMDYZFMcud"
      },
      "outputs": [],
      "source": [
        "#Train models and make predictions.\n",
        "max_splits = 10\n",
        "n_splits = range(2,max_splits)\n",
        "kfcv_filename = 'kfcv_{}'.format(max_splits)\n",
        "kfcv_filename_path = os.path.join(exe04_tables_path, kfcv_filename)\n",
        "\n",
        "if os.path.isfile(kfcv_filename_path+\".pkl\"):\n",
        "    # load existing model\n",
        "    print(\"\\n{}: loading k-Fold Cross-Validation Scores:\\t{}\".format(timenow(), kfcv_filename))\n",
        "    kfcv_df = pd.read_csv(kfcv_filename_path+\".csv\", index_col=0)\n",
        "    kfcv_file = open(kfcv_filename_path+\".pkl\", \"rb\")\n",
        "    model_kfcv_results = pickle.load(kfcv_file)\n",
        "    kfcv_means_df = pd.DataFrame(index=model_kfcv_results.keys(), columns=n_splits)\n",
        "    for model_str, kfcv_results in model_kfcv_results.items():\n",
        "        print(\"\\n{}: Model: {}\".format(timenow(), model_str))\n",
        "        for score in kfcv_results:\n",
        "            print(\"{}:  N splits: {}\".format(timenow(),len(score)))\n",
        "            print(\"{}:    Accuracy scores: {}\\n\\t     Mean (std): \\t{:.3f} ({:.5f})\".format(timenow(),score.round(3), np.mean(score), sem(score)))\n",
        "            kfcv_means_df.loc[model_str,len(score)] = np.mean(score)\n",
        "        plt.boxplot(kfcv_results, showmeans=True, labels=n_splits)\n",
        "        plt.xlabel(\"Nº splits\")\n",
        "        plt.ylabel(\"Accuracy score\")\n",
        "        plt.title(model_str, fontweight = \"bold\")\n",
        "        filename = model_str+\"-K-fold_cv.png\"\n",
        "        plt.savefig(os.path.join(exe04_plots_path, filename))\n",
        "        plt.show()\n",
        "else:    \n",
        "    model_kfcv_results = dict()\n",
        "    for model in selected_classifiers:\n",
        "        model_str = str(model).split(\"(\")[0]\n",
        "        print(\"\\n{}: Model: {}\".format(timenow(), model_str))\n",
        "        kfcv_results = list()\n",
        "        for n_s in n_splits:\n",
        "            print(\"{}:  N splits: {}\".format(timenow(),n_s))\n",
        "            # evaluate using a given number of repeats\n",
        "            scores = kfcv_evaluator_class(X, y,model, n_s)\n",
        "            # summarize\n",
        "            print(\"{}:    Accuracy scores: {}\\n\\t     Mean (std): \\t{:.3f} ({:.5f})\".format(timenow(),scores.round(3), np.mean(scores), sem(scores)))\n",
        "            # store\n",
        "            kfcv_results.append(scores)\n",
        "        model_kfcv_results[model_str] = kfcv_results\n",
        "\n",
        "        plt.boxplot(kfcv_results, showmeans=True, labels=n_splits)\n",
        "        plt.xlabel(\"Nº splits\")\n",
        "        plt.ylabel(\"Accuracy score\")\n",
        "        plt.title(model_str, fontweight = \"bold\")\n",
        "        \n",
        "        filename = model_str+\"-K-fold_cv.png\"\n",
        "        plt.savefig(os.path.join(exe04_plots_path, filename))\n",
        "        plt.show()\n",
        "    \n",
        "    kfcv_file = open(kfcv_filename_path+\".pkl\", \"wb\")\n",
        "    pickle.dump(model_kfcv_results, kfcv_file)\n",
        "    kfcv_file.close()\n",
        "    kfcv_df = pd.DataFrame.from_dict(model_kfcv_results, orient='index', columns=n_splits)\n",
        "    kfcv_df.to_csv(kfcv_filename_path+\".csv\")\n",
        "\n",
        "display(kfcv_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   We calculate the mean of the results obtained in each model and by number of splits.\n",
        "2. The results are plotted to see the effect of the number of splits on each model."
      ],
      "metadata": {
        "id": "oVzClGnRBHsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfcv_file = open(kfcv_filename_path+\".pkl\", \"rb\")\n",
        "model_kfcv_results = pickle.load(kfcv_file)\n",
        "\n",
        "kfcv_means_df = pd.DataFrame(index=model_kfcv_results.keys(), columns=n_splits)\n",
        "for model_str, kfcv_results in model_kfcv_results.items():\n",
        "    for score in kfcv_results:\n",
        "        kfcv_means_df.loc[model_str,len(score)] = np.mean(score)\n",
        "kfcv_means_df.to_csv(kfcv_filename_path+\"_means.csv\")\n",
        "kfcv_means_df = kfcv_means_df.round(3)\n",
        "display(kfcv_means_df)\n",
        "\n",
        "for model in kfcv_means_df.index:\n",
        "    plt.plot(kfcv_means_df.loc[model], label = model)\n",
        "\n",
        "plt.xlabel(\"Nº splits (K-folds)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"K-Fold Cross-Validation results\\n Scores by number of splits \")\n",
        "plt.legend(title=\"Models\",loc='upper right', bbox_to_anchor=(0.7, 0.6, 0.5, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nv0mGMRV8FmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find that by means of this internal validation method we do obtain better results than by train test approach.\n",
        "\n",
        "We see that increasing the number of k has a small positive effect."
      ],
      "metadata": {
        "id": "qmx8k0EAKL1K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbA5Dtf7jjdp"
      },
      "source": [
        "## Repeated k-Fold Cross-Validation (internal validation)\n",
        "\n",
        "Repeats K-Fold n times with different randomization in each repetition.\n",
        "\n",
        "We will be increasing the K from 2 to 4  and the number or repetitions from 1 to 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdp6hHnMeuSe"
      },
      "outputs": [],
      "source": [
        "#Train models and make predictions.\n",
        "\n",
        "max_splits = 5\n",
        "max_repeats = 4\n",
        "n_splits = range(2,max_splits)\n",
        "n_repeats = range(1,max_repeats)\n",
        "\n",
        "rkfcv_filename = 'rkfcv_{}_{}'.format(max_splits,max_repeats)\n",
        "rkfcv_filename_path = os.path.join(exe04_tables_path, rkfcv_filename)\n",
        "rkfcv_results = dict()\n",
        "for model in selected_classifiers:\n",
        "    model_str = str(model).split(\"(\")[0]\n",
        "    model_filename = rkfcv_filename_path+'-'+model_str+\".sav\"\n",
        "    print(\"\\n{}: Model: {}\".format(timenow(), model_str))\n",
        "    if os.path.isfile(model_filename):\n",
        "        print(\"\\n{}: loading Repeated k-Fold Cross-Validation Scores:\\t{}\".format(timenow(), model_filename))\n",
        "        model_file = open(model_filename, \"rb\")\n",
        "        model_rkfcv_scores = pickle.load(model_file)\n",
        "        model_file.close()\n",
        "        rkfcv_results[model_str] = model_rkfcv_scores\n",
        "    else:\n",
        "        model_rkfcv_scores = dict()\n",
        "        for n_s in n_splits:\n",
        "            print(\"{}:\\tN splits: {}\".format(timenow(),n_s))\n",
        "            reps_rkfcv_results = dict() \n",
        "            for n_r in n_repeats:\n",
        "                print(\"{}:\\t\\tN repeats: {}\".format(timenow(),n_r))\n",
        "                scores = rkfcv_evaluator_class(X, y,model, n_s, n_r)\n",
        "                print(\"{}:\\t\\t\\tAccuracy scores:\\t{}\\n\\t\\t\\t\\tMean (std):\\t{:.6f} ({:.6f})\".format(timenow(),scores, np.mean(scores), sem(scores)))\n",
        "                reps_rkfcv_results[n_r] = scores\n",
        "            model_rkfcv_scores[n_s] = reps_rkfcv_results\n",
        "        model_file = open(model_filename, \"wb\")\n",
        "        pickle.dump(model_rkfcv_scores, model_file)\n",
        "        model_file.close()\n",
        "        rkfcv_results[model_str] = model_rkfcv_scores\n",
        "\n",
        "rkfcv_df = rkfcv_to_df(rkfcv_results)\n",
        "display(rkfcv_df)\n",
        "plot_rkfcv(rkfcv_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   We calculate the mean of the results obtained in each model and by number of splits and number of repeats.\n",
        "2. The results are plotted to see the effect of the number of repeats on each model."
      ],
      "metadata": {
        "id": "hFdsXoqbIklV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rkfcv_df.loc[:,\"1\":] = rkfcv_df.loc[:,\"1\":].applymap(np.mean)\n",
        "\n",
        "for model in selected_classifiers:\n",
        "    model_str = str(model).split(\"(\")[0]\n",
        "    print(model_str)\n",
        "    rkfcv_model_results = rkfcv_df[rkfcv_df.model ==model_str]\n",
        "    rkfcv_model_results.drop('model',axis=1, inplace=True)\n",
        "    rkfcv_model_results.set_index('n_splits', inplace=True)\n",
        "    sns.lineplot(data=rkfcv_model_results.T)\n",
        "    plt.xlabel(\"Nº Repeats\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"{}\\nRepeated K-Fold Cross-Validation results\\n Scores by number of Repeats hued by number of splits\".format(model_str))\n",
        "    plt.legend(title=\"Nº Splits\",loc='upper right', bbox_to_anchor=(0.55, 0.5, 0.5, 0.5))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KB5phejXHnqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the same number of splits, in any model increasing repetitions does not generate better results but in the model `KNeighborsClassifier`\n"
      ],
      "metadata": {
        "id": "TXi3NKskMg3k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piDs7vJTEHpI"
      },
      "source": [
        "# Exercise 5: \n",
        "  - Perform some variable engineering process to improve prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0_gXaUL06O_"
      },
      "outputs": [],
      "source": [
        "mk_dir(\"exe05\")\n",
        "exe05_models_path = mk_dir(\"exe05/exe05_models\")\n",
        "exe05_tables_path = mk_dir(\"exe05/exe05_tables\")\n",
        "exe05_plots_path = mk_dir(\"exe05/exe05_plots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UqoETCX_L_S"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"DelayedFlights_Processed.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM3pVEiL3daj"
      },
      "source": [
        "\n",
        "From \"DepTime\" we extract the hour and minutes and assemble them with the variables \"Year\", \"Month\" and \"DayofMonth\" to then convert DepTime to datetime format (YYYY-MM-DD HH:MM:SS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhGeQmF34yGe"
      },
      "outputs": [],
      "source": [
        "df[\"DepTime\"] = pd.to_datetime(dict(year=df.Year,\n",
        "                                    month=df.Month,\n",
        "                                    day=df.DayofMonth,\n",
        "                                    hour=[t.hour for t in df.DepTime.apply(numToTime)],\n",
        "                                    minute=[t.minute for t in df.DepTime.apply(numToTime)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtsN5wbx5RE4"
      },
      "source": [
        "Convert \"DepDelay\" to timedelta, so we can use it to obtain absolute differences in times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5vrpBRb5hU8"
      },
      "outputs": [],
      "source": [
        "df[\"DepDelay\"] = pd.to_timedelta(df[\"DepDelay\"], unit='m')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9TVLI6G7CP4"
      },
      "source": [
        "Now we can get the \"CRSDepTime\" in datetime format by subtracting the \"DepDelay\" from the \"DepTime\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uk_BXb17GnM"
      },
      "outputs": [],
      "source": [
        "df[\"CRSDepTime\"] = df[\"DepTime\"] - df[\"DepDelay\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31A4LzbO7cA3"
      },
      "source": [
        "Now we to transform the \"DepDelay\" from timedelta back to minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGEIutRZ7kCW"
      },
      "outputs": [],
      "source": [
        "df[\"DepDelay\"] = df.DepDelay.dt.seconds/60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFQEP0FR8zS-"
      },
      "source": [
        "We sort the records, reindex and eliminate the variables \"Year\", \"Month\", \"DayofMonth\", \"DayOfWeek\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aryf5OGdA55K"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by=[\"CRSDepTime\",\"DepTime\"], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df = df.drop([\"Year\",\"Month\",\"DayofMonth\",\"DayOfWeek\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayRKFxEuAikW"
      },
      "source": [
        "Now that we have \"DepTime\" and \"CRSDepTime\" in datetime format we can extract new features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuMebEimHj2t"
      },
      "source": [
        "Extract cyclic time related features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cq53YTKPvvA"
      },
      "outputs": [],
      "source": [
        "df[\"DepTime_hour\"] = df.DepTime.dt.hour\n",
        "df[\"DepTime_minute\"] = df.DepTime.dt.minute\n",
        "df[\"DepTime_month\"] = df.DepTime.dt.month\n",
        "df[\"DepTime_day\"] = df.DepTime.dt.day\n",
        "df[\"DepTime_dayofweek\"] = df.DepTime.dt.dayofweek\n",
        "df[\"DepTime_weekofyear\"] = df.DepTime.dt.weekofyear\n",
        "df[\"DepTime_dayofyear\"] = df.DepTime.dt.dayofyear\n",
        "df[\"DepTime_quarter\"] = df.DepTime.dt.quarter\n",
        "\n",
        "df[\"CRSDepTime_day\"] = df.CRSDepTime.dt.day\n",
        "df[\"CRSDepTime_hour\"] = df.CRSDepTime.dt.hour\n",
        "df[\"CRSDepTime_minute\"] = df.CRSDepTime.dt.minute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwo2FUBqH2nA"
      },
      "source": [
        "Extract number of days in the month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOBF0tBzHsO8"
      },
      "outputs": [],
      "source": [
        "df[\"DepTime_daysinmonth\"] = df.DepTime.dt.daysinmonth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z84aov9mHuFk"
      },
      "source": [
        "Extract bolean features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwXKMMClHtLN"
      },
      "outputs": [],
      "source": [
        "df[\"DepTime_is_month_start\"] = df.DepTime.dt.is_month_start\n",
        "df[\"DepTime_is_month_end\"] = df.DepTime.dt.is_month_end\n",
        "df[\"DepTime_is_quarter_start\"] = df.DepTime.dt.is_quarter_start\n",
        "df[\"DepTime_is_quarter_end\"] = df.DepTime.dt.is_quarter_end\n",
        "df[\"DepTime_is_year_start\"] = df.DepTime.dt.is_year_start\n",
        "df[\"DepTime_is_year_end\"] = df.DepTime.dt.is_year_end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihVaNoOsH99m"
      },
      "source": [
        "Now we can drop \"DepTime\",\"CRSDepTime\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDQ1hS8wH8vw"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"DepTime\",\"CRSDepTime\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2akC0M5ILcK"
      },
      "source": [
        "Convert the boolean values into integer value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1v0a--WpL-S"
      },
      "outputs": [],
      "source": [
        "bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
        "df[bool_cols] =  df[bool_cols].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scyO6AsDJo7V"
      },
      "source": [
        "Encode cyclical features such as all time related features into an angular distance by calculating the cosinus and sinus values of the degree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3hvbqYGxu6X"
      },
      "outputs": [],
      "source": [
        "cyclic_cols = ['ArrTime', 'CRSArrTime','DepTime_month', 'DepTime_day', \n",
        "               'DepTime_hour', 'DepTime_minute', 'DepTime_weekofyear', \n",
        "               'DepTime_dayofweek', 'DepTime_dayofyear', 'DepTime_quarter', \n",
        "               'CRSDepTime_day', 'CRSDepTime_hour', 'CRSDepTime_minute']\n",
        "               \n",
        "cyclical = CyclicalTransformer(variables=cyclic_cols, drop_original=True)\n",
        "df = cyclical.fit_transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop Categorical Variables we are not going to use"
      ],
      "metadata": {
        "id": "_jFUUWq0JLk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"TailNum\",\"Origin\", \"Dest\", \"FlightNum\"], axis=1)"
      ],
      "metadata": {
        "id": "od1bmmvfTctx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HJarK1LWJ_O"
      },
      "source": [
        "Transform Categorical Variables (\"UniqueCarrie\") to dummies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df)"
      ],
      "metadata": {
        "id": "oXzOASmFcJWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQziqwMCbBG6"
      },
      "source": [
        "Save engineered data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs_0D-YzbBG6"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"DelayedFlights_Engineered.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide data into Explanatory and Response variables. "
      ],
      "metadata": {
        "id": "poPu4nDIJvHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['ArrDelay'], axis = 'columns')\n",
        "y = df['ArrDelay']"
      ],
      "metadata": {
        "id": "QpR9RbypgOwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We scale the data and divide it into training and test sets"
      ],
      "metadata": {
        "id": "J10uwnLJKBu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = MinMaxScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "IzNyUCRFgOwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=130)"
      ],
      "metadata": {
        "id": "DIFAT-sEgOwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform over-sampling using SMOTE."
      ],
      "metadata": {
        "id": "mhf4yUzhNCbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(random_state=130)\n",
        "X_train, y_train = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "UCRoebmKUVJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train models and make predictions."
      ],
      "metadata": {
        "id": "v_8TMJpeNr42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_lst = []\n",
        "for model in classifiers:\n",
        "    # Model name \n",
        "    model_str = str(model).split(\"(\")[0]\n",
        "    model_name = '{}-{}'.format(\"05\", model_str)\n",
        "    # File name\n",
        "    model_filename = model_name + \".pkl\"\n",
        "    model_filename_path = os.path.join(exe05_models_path, model_filename)\n",
        "    if os.path.isfile(model_filename_path):\n",
        "        # load existing model\n",
        "        print(\"\\n{}: Loading model:\\t{}\".format(timenow(), model_filename))\n",
        "        model = pickle.load(open(model_filename_path, 'rb'))\n",
        "    else:\n",
        "        # Fit model\n",
        "        print(\"\\n{}: Start Fitting model:\\t{}\".format(timenow(), model_name))\n",
        "        tiempo_inicio_fit = datetime.datetime.now()\n",
        "        model.fit(X_train, y_train)\n",
        "        fit_time = datetime.datetime.now() - tiempo_inicio_fit\n",
        "        print(\"Fitting time:\\t{}\".format(fit_time))\n",
        "\n",
        "        # save the model to disk\n",
        "        pickle.dump(model, open(model_filename_path, 'wb'))\n",
        "\n",
        "    # Evaluate model\n",
        "    plt_path = os.path.join(exe05_plots_path, model_name + \"_conf.png\")\n",
        "    print(\"\\n{}: Evaluate model:\\n\".format(timenow()))\n",
        "    metrics_lst.append(class_metrics(model, model_name, X_train, X_test, y_train, y_test, plt_path))\n",
        "# Tabualate resulst\n",
        "class_metrics_df = pd.DataFrame(metrics_lst)\n",
        "class_metrics_df.set_index(keys=\"Model\", inplace=True)\n",
        "class_metrics_df.sort_values(by=[\"Accuracy\"], ascending=False, inplace=True)\n",
        "# Save results\n",
        "class_metrics_df.to_csv(os.path.join(exe05_tables_path, \"05-classification_metrics.csv\"))"
      ],
      "metadata": {
        "id": "SXxmarAzU447"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the effect of engineering on the predictions we will compare the results with those of exercise 1.\n",
        "\n",
        "We combine the results of the two exercises."
      ],
      "metadata": {
        "id": "0VVbq4S2trA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_01_path = os.path.join(exe01_tables_path, \"01-classification_metrics.csv\")\n",
        "metrics_05_path = os.path.join(exe05_tables_path, \"05-classification_metrics.csv\")\n",
        "\n",
        "metrics_lst = []\n",
        "for metrics_file in [metrics_01_path, metrics_05_path]:\n",
        "    \n",
        "    all_metrics = pd.read_csv(metrics_file, index_col=0)\n",
        "    metrics_lst.append(all_metrics)\n",
        "final_metrics_df_1_5 = pd.concat(metrics_lst)\n",
        "\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(final_metrics_df_1_5, num_rows_per_page=final_metrics_df_1_5.shape[0])"
      ],
      "metadata": {
        "id": "54aX7UVJGpRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We print the model that obtains the best metric."
      ],
      "metadata": {
        "id": "Oo0iGnINvHg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in classifiers:\n",
        "    # Model name \n",
        "    model_str = str(model).split(\"(\")[0]\n",
        "    models_metric = final_metrics_df_1_5.filter(regex=model_str, axis=0)\n",
        "    print(\"\\n{}: \".format(model_str))\n",
        "    print(\"{:<15} {:<10}\".format(\"METRIC\",\"BEST MODEL\"))\n",
        "    print(\"{:<15} {:<10}\".format(\"-\"*10,\"-\"*10))\n",
        "    for metric in models_metric.columns:\n",
        "        if metric in [\"FP_rate\",\"Misclass_rate\",\"Misclass\",\"FP\",\"FN\"]:\n",
        "            ascend=True\n",
        "        else:\n",
        "            ascend=False\n",
        "        models_metric.sort_values(by=metric, inplace=True, ascending=ascend)\n",
        "        print(\"{:<15} {:<10}\".format(metric,models_metric.head(1).index[0]))"
      ],
      "metadata": {
        "id": "cpL9ZkDLpA_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each metric we plot the models ordered from best to worst result."
      ],
      "metadata": {
        "id": "roytsw1suiLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for metric in final_metrics_df_1_5.columns:\n",
        "    if metric in [\"FP_rate\",\"Misclass_rate\",\"Misclass\",\"FP\",\"FN\"]:\n",
        "        ascend=True\n",
        "    else:\n",
        "        ascend=False\n",
        "    final_metrics_df_1_5.sort_values(by=metric, inplace=True, ascending=ascend)\n",
        "    plt.figure(figsize = (15,final_metrics_df_1_5.shape[0]/4))\n",
        "    sns.barplot(data=final_metrics_df_1_5, x = metric, y = final_metrics_df_1_5.index)\n",
        "    plt.title(metric)\n",
        "    plt_path = os.path.join(exe05_plots_path, metric + \"_summary.png\")\n",
        "    plt.savefig(plt_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KOGKAX4pXDDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bou0kIAYZsw8"
      },
      "source": [
        "# Exercise 6: \n",
        "  - Do not use the DepDelay variable when making predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyLhIoVDaD90"
      },
      "outputs": [],
      "source": [
        "mk_dir(\"exe06\")\n",
        "exe06_models_path = mk_dir(\"exe06/exe06_models\")\n",
        "exe06_tables_path = mk_dir(\"exe06/exe06_tables\")\n",
        "exe06_plots_path = mk_dir(\"exe06/exe06_plots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAf35wruaD91"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"DelayedFlights_Engineered.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['ArrDelay', 'DepDelay'], axis = 'columns')\n",
        "y = df['ArrDelay']"
      ],
      "metadata": {
        "id": "A0PcblygcRw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We scale the data and divide it into training and test sets"
      ],
      "metadata": {
        "id": "xXqXan8-KE4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = MinMaxScaler().fit_transform(X)"
      ],
      "metadata": {
        "id": "mgNgJfKTcRw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=130)"
      ],
      "metadata": {
        "id": "X7r3ZAxOcRw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform over-sampling using SMOTE."
      ],
      "metadata": {
        "id": "RbK5kniLNobi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(random_state=130)\n",
        "X_train, y_train = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "OzUGYSfNcRw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train models and make predictions."
      ],
      "metadata": {
        "id": "uKc3p72kOAM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_lst = []\n",
        "for model in classifiers:\n",
        "    # Model name \n",
        "    model_str = str(model).split(\"(\")[0]\n",
        "    model_name = '{}-{}'.format(\"06\", model_str)\n",
        "    # File name\n",
        "    model_filename = model_name + \".pkl\"\n",
        "    model_filename_path = os.path.join(exe06_models_path, model_filename)\n",
        "    if os.path.isfile(model_filename_path):\n",
        "        # load existing model\n",
        "        print(\"\\n{}: Loading model:\\t{}\".format(timenow(), model_filename))\n",
        "        model = pickle.load(open(model_filename_path, 'rb'))\n",
        "    else:\n",
        "        # Fit model\n",
        "        print(\"\\n{}: Start Fitting model:\\t{}\".format(timenow(), model_name))\n",
        "        tiempo_inicio_fit = datetime.datetime.now()\n",
        "        model.fit(X_train, y_train)\n",
        "        fit_time = datetime.datetime.now() - tiempo_inicio_fit\n",
        "        print(\"Fitting time:\\t{}\".format(fit_time))\n",
        "\n",
        "        # save the model to disk\n",
        "        pickle.dump(model, open(model_filename_path, 'wb'))\n",
        "\n",
        "    # Evaluate model\n",
        "    plt_path = os.path.join(exe06_plots_path, model_name + \"_conf.png\")\n",
        "    print(\"\\n{}: Evaluate model:\\n\".format(timenow()))\n",
        "    metrics_lst.append(class_metrics(model, model_name, X_train, X_test, y_train, y_test, plt_path))\n",
        "# Tabualate resulst\n",
        "class_metrics_df = pd.DataFrame(metrics_lst)\n",
        "class_metrics_df.set_index(keys=\"Model\", inplace=True)\n",
        "class_metrics_df.sort_values(by=[\"Accuracy\"], ascending=False, inplace=True)\n",
        "# Save results\n",
        "class_metrics_df.to_csv(os.path.join(exe06_tables_path, \"06-classification_metrics.csv\"))"
      ],
      "metadata": {
        "id": "21x2brAzaD99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the effects of missing the \"DepDelay\" variable on the predictions we will compare the results with those of exercise 5.\n",
        "\n",
        "We combine the results of the two exercises."
      ],
      "metadata": {
        "id": "RYK_zwzwwKcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_05_path = os.path.join(exe05_tables_path, \"05-classification_metrics.csv\")\n",
        "metrics_06_path = os.path.join(exe06_tables_path, \"06-classification_metrics.csv\")\n",
        "metrics_lst = []\n",
        "for metrics_file in [metrics_06_path, metrics_05_path]:\n",
        "    \n",
        "    all_metrics = pd.read_csv(metrics_file, index_col=0)\n",
        "    metrics_lst.append(all_metrics)\n",
        "final_metrics_df_5_6 = pd.concat(metrics_lst)\n",
        "\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(final_metrics_df_5_6, num_rows_per_page=final_metrics_df_5_6.shape[0])"
      ],
      "metadata": {
        "id": "xKkpNZ7saD9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We print the model that obtains the best metric."
      ],
      "metadata": {
        "id": "3ov6rtqMwqD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in classifiers:\n",
        "    # Model name \n",
        "    model_str = str(model).split(\"(\")[0]\n",
        "    models_metric = final_metrics_df_5_6.filter(regex=model_str, axis=0)\n",
        "    print(\"\\n{}: \".format(model_str))\n",
        "    print(\"{:<15} {:<10}\".format(\"METRIC\",\"BEST MODEL\"))\n",
        "    print(\"{:<15} {:<10}\".format(\"-\"*10,\"-\"*10))\n",
        "    for metric in models_metric.columns:\n",
        "        if metric in [\"FP_rate\",\"Misclass_rate\",\"Misclass\",\"FP\",\"FN\"]:\n",
        "            ascend=True\n",
        "        else:\n",
        "            ascend=False\n",
        "        models_metric.sort_values(by=metric, inplace=True, ascending=ascend)\n",
        "        print(\"{:<15} {:<10}\".format(metric,models_metric.head(1).index[0]))"
      ],
      "metadata": {
        "id": "v_zcC6SWwqD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each metric we plot the models ordered from best to worst result."
      ],
      "metadata": {
        "id": "bZr-tuITw6bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for metric in final_metrics_df_5_6.columns:\n",
        "    if metric in [\"FP_rate\",\"Misclass_rate\",\"Misclass\",\"FP\",\"FN\"]:\n",
        "        ascend=True\n",
        "    else:\n",
        "        ascend=False\n",
        "    final_metrics_df_5_6.sort_values(by=metric, inplace=True, ascending=ascend)\n",
        "    plt.figure(figsize = (15,final_metrics_df_5_6.shape[0]/4))\n",
        "    sns.barplot(data=final_metrics_df_5_6, x = metric, y = final_metrics_df_5_6.index)\n",
        "    plt.title(metric)\n",
        "    plt_path = os.path.join(exe06_plots_path, metric + \"_summary.png\")\n",
        "    plt.savefig(plt_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bgff08EfhwO4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}